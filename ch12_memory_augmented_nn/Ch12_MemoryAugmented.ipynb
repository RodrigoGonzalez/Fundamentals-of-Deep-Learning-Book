{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ch13_MemoryAugmented.ipynb","provenance":[],"authorship_tag":"ABX9TyMN61Lo3iRYxZllKljd3GDP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Implementing the DNC in PyTorch\n","See archive/dnc/mem_ops.py:\n","\n","## TO-DO implement test code for files\n"],"metadata":{"id":"AmTCQQ41h1gs"}},{"cell_type":"code","source":["import torch\n","import numpy as np"],"metadata":{"id":"o9e2gyT6JZya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def init_memory(N, W, R):\n","    \"\"\"\n","    returns the initial values of the memory matrix, usage vector,\n","    precedence vector, link matrix, read weightings, write weightings,\n","    and the read vectors\n","    \"\"\"\n","\n","    M0 = torch.fill([N, W], 1e-6)\n","    u0 = torch.zeros([N])\n","    p0 = torch.zeros([N])\n","    L0 = torch.zeros([N, N])\n","    wr0 = torch.fill([N, R], 1e-6)  # initial read weightings\n","    ww0 = torch.fill([N], 1e-6)  # initial write weightings\n","    r0 = torch.fill([W, R], 1e-6)  # initial read vector\n","\n","    return M0, u0, p0, L0, wr0, ww0, r0"],"metadata":{"id":"qa7vsyhyJUts"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def parse_interface(zeta, N, W, R):\n","    \"\"\"\n","    returns the individual components of the interface vector\n","    \"\"\"\n","    cursor = 0  # keeps track of how far we parsed into zeta\n","    kr, cursor = torch.reshape(zeta[cursor:cursor + W*R], [W, R]), cursor + W*R\n","    br, cursor = zeta[cursor:cursor + R], cursor + R\n","    kw, cursor = torch.reshape(zeta[cursor: cursor + W], [W, 1]), cursor + W\n","    bw, cursor = zeta[cursor], cursor + 1\n","    e, cursor = zeta[cursor: cursor + W], cursor + W\n","    v, cursor = zeta[cursor: cursor + W], cursor + W\n","    f, cursor = zeta[cursor: cursor + R], cursor + R\n","    ga, cursor = zeta[cursor], cursor + 1\n","    gw, cursor = zeta[cursor], cursor + 1\n","    pi = torch.reshape(zeta[cursor:], [3, R])\n","\n","    # transforming the parsed components into their correct values\n","    oneplus = lambda z: 1 + torch.nn.softplus(z)\n","\n","    e = torch.nn.sigmoid(e)\n","    f = torch.nn.sigmoid(f)\n","    ga = torch.nn.sigmoid(ga)\n","    gw = torch.nn.sigmoid(gw)\n","    br = oneplus(br)\n","    bw = oneplus(bw)\n","    pi = torch.nn.softmax(pi, 0)\n","\n","    return kr, br, kw, bw, e, v, f, ga, gw, pi"],"metadata":{"id":"NRIWtxAOJiHx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def C(M, k, b):\n","    \"\"\"\n","    Content-based addressing weightings\n","    \"\"\"\n","    M_normalized = torch.nn.l2_normalize(M, 1)\n","    k_normalized = torch.nn.l2_normalize(k, 0)\n","    similarity = torch.matmul(M_normalized, k_normalized)\n","\n","    return torch.nn.softmax(similarity * b, 0)"],"metadata":{"id":"FJV6qe3xJq-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def ut(u, f, wr, ww):\n","    \"\"\"\n","    returns the updated usage vector given the previous one along with\n","    free gates and previous read and write weightings\n","    \"\"\"\n","    psi_t = torch.reduce_prod(1 - f * wr, 1)\n","    return (u + ww - u * ww) * psi_t\n"],"metadata":{"id":"xMXnPHrmJv-S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def at(ut, N):\n","    \"\"\"\n","    returns the allocation weighting given the updated usage vector\n","    \"\"\"\n","    sorted_ut, free_list = torch.nn.top_k(-1 * ut, N)\n","    sorted_ut *= -1  # brings the usages to the original positive values\n","\n","    # the exclusive argument makes the first element in the cumulative\n","    # product a 1 instead of the first element in the given tensor\n","    sorted_ut_cumprod = torch.cumprod(sorted_ut, exclusive=True)\n","    out_of_location_at = (1 - sorted_ut) * sorted_ut_cumprod\n","\n","    empty_at_container = torch.TensorArray(torch.float32, N)\n","    full_at_container = empty_at_container.scatter(free_list, out_of_location_at)\n","\n","    return full_at_container.pack()"],"metadata":{"id":"UftcaEQoJz5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def wwt(ct, at, gw, ga):\n","    \"\"\"\n","    returns the upadted write weightings given allocation and content-based\n","    weightings along with the write and allocation gates\n","    \"\"\"\n","    ct = torch.squeeze(ct)\n","    return gw * (ga * at + (1 - ga) * ct)"],"metadata":{"id":"ml5f0as2J5TR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["--------------------------------------------------------------------"],"metadata":{"id":"23Oe11iynJLl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Lt(L, wwt, p, N):\n","    \"\"\"\n","    returns the updated link matrix given the previous one along \n","    with the updated write weightings and the previous precedence \n","    vector\n","    \"\"\"\n","    def pairwise_add(v):\n","        \"\"\"\n","        returns the matrix of pairs - adding the elements of v to \n","        themselves\n","        \"\"\"\n","        n = v.get_shape().as_list()[0]\n","        # a NxN matrix of duplicates of u along the columns\n","        V = torch.concat(1, [v] * n)  \n","        return V + V\n","\n","    # expand dimensions of wwt and p to make matmul behave as outer \n","    # product\n","    wwt = torch.expand_dims(wwt, 1)\n","    p = torch.expand_dims(p, 0)\n","\n","    I = torch.constant(np.identity(N, dtype=np.float32))\n","    return (((1 - pairwise_add(wwt)) * L + \n","             torch.matmul(wwt, p)) * (1 - I))\n"],"metadata":{"id":"2H6AyNw6J8nN","executionInfo":{"status":"ok","timestamp":1641996847845,"user_tz":300,"elapsed":140,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def pt(wwt, p):\n","    \"\"\"\n","    returns the updated precedence vector given the new write weightings and\n","    the previous precedence vector\n","    \"\"\"\n","    return (1 - torch.reduce_sum(wwt)) * p + wwt"],"metadata":{"id":"0P6lZSnYKFqx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def Mt(M, wwt, e, v):\n","    \"\"\"\n","    returns the updated memory matrix given the previous one, the new write\n","    weightings, and the erase and write vectors\n","    \"\"\"\n","    # expand the dims of wwt, e, and v to make matmul\n","    # behave as outer product\n","    wwt = torch.expand_dims(wwt, 1)\n","    e = torch.expand_dims(e, 0)\n","    v = torch.expand_dims(v, 0)\n","\n","    return M * (1 - torch.matmul(wwt, e)) + torch.matmul(wwt, v)"],"metadata":{"id":"UcvCmDhNKJTM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def wrt(wr, Lt, ct, pi):\n","    \"\"\"\n","    returns the updated read weightings given the previous ones, the new link\n","    matrix, a content-based weighting, and the read modes\n","    \"\"\"\n","    ft = torch.matmul(Lt, wr)\n","    bt = torch.matmul(Lt, wr, transpose_a=True)\n","\n","    return pi[0] * bt + pi[1] * ct + pi[2] * ft"],"metadata":{"id":"GUU8SEZMKMzQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rt(Mt, wrt):\n","    \"\"\"\n","    returns the new read vectors given the new memory matrix and the new read\n","    weightings\n","    \"\"\"\n","    return torch.matmul(Mt, wrt, transpose_a=True)"],"metadata":{"id":"Fm9DNV1oKVDB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"X_7p3ypsKYHM"},"execution_count":null,"outputs":[]}]}