{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ch09_03_LSTM_Sentiment.ipynb","provenance":[{"file_id":"1Fvtvxci3HGvdeBvUYDZd_mDuRfZ_rx2b","timestamp":1641846406867},{"file_id":"1XL97FXDkJDFMjM4M_FjCslCrdcanzDE8","timestamp":1641846356338}],"collapsed_sections":[],"authorship_tag":"ABX9TyPZFGEZLSbt9vQO55mBeulL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Implementing a Sentiment Analysis Model"],"metadata":{"id":"rXkUdXjimRyb"}},{"cell_type":"markdown","source":["- IMDB Movie Reviews, Binary Pos/Neg Sentiment\n","- Prune vocabulary to 30,000 most common words\n","- Pad each input sequence up to 500 words\n","- Inputs should be 500 dim vectors where each element correspnds to the vocab index of the corresponding word"],"metadata":{"id":"ToALc452WOJt"}},{"cell_type":"code","source":["import torch\n","from torchtext.datasets import IMDB\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.data.utils import get_tokenizer"],"metadata":{"id":"XtJEcMDPuhTt","executionInfo":{"status":"ok","timestamp":1642087516071,"user_tz":300,"elapsed":1157,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["#@title\n","def build_vocab():"],"metadata":{"cellView":"form","id":"SRAM-4WGCPUO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title\n","text_vocab = build_vocab(yield_tokens(train_iter),\n","                         max_size=30000, \n","                         specials=['<unk>', '<pad>'])\n","text_vocab.set_default_index(text_vocab['<unk>'])\n"],"metadata":{"cellView":"form","id":"WwM6MXwMB4bY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset\n","train_iter = IMDB(split=('train'))"],"metadata":{"id":"7-gu9IfAx1kH","executionInfo":{"status":"ok","timestamp":1642087537038,"user_tz":300,"elapsed":9776,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Define tokenizer and build vocabulary\n","tokenizer = get_tokenizer('basic_english')\n","\n","def yield_tokens(data_iter):\n","    for _, text in data_iter:\n","        yield tokenizer(text)\n","\n","# build vocab from iterator and add a list of any special tokens\n","text_vocab = build_vocab_from_iterator(yield_tokens(train_iter), \n","                                       specials=['<unk>', '<pad>'])\n","text_vocab.set_default_index(text_vocab['<unk>'])"],"metadata":{"id":"XU4RVcFJ3TSw","executionInfo":{"status":"ok","timestamp":1642087548225,"user_tz":300,"elapsed":6988,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["print(text_vocab(tokenizer(\"Hello is it me you're looking for?\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vCSX9_hFC4W_","executionInfo":{"status":"ok","timestamp":1642087603640,"user_tz":300,"elapsed":156,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}},"outputId":"86ecac87-a787-4243-85a3-ef20e5768992"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[4645, 10, 11, 78, 26, 9, 183, 296, 19, 55]\n"]}]},{"cell_type":"code","source":["# Alternate way\n","# rewrite build_vocab_from_iterator\n","# https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab_factory.py"],"metadata":{"id":"tmTeYq0__TU6","executionInfo":{"status":"ok","timestamp":1642087274570,"user_tz":300,"elapsed":143,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# # Define tokenizer and build vocabulary\n","# tokenizer = get_tokenizer('basic_english')\n","\n","# def yield_tokens(data_iter):\n","#     for _, text in data_iter:\n","#         yield tokenizer(text)\n","\n","# # build vocab from iterator and add a list of any special tokens\n","# full_vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<pad>'])\n","# full_vocab.set_default_index(full_vocab['<unk>'])\n","# text_vocab = lambda x: full_vocab(x) if full_vocab.get_stoi()[x] < 30000 else '<unk>'"],"metadata":{"id":"qiI2AmoR7UpB","executionInfo":{"status":"ok","timestamp":1642087275905,"user_tz":300,"elapsed":133,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# print(full_vocab.get_itos()[29999])\n","# print(full_vocab.get_itos()[30000])\n","# #print(text_vocab(full_vocab.get_itos()[29999]))\n","# print(text_vocab('wanderings'))\n","# print(text_vocab(full_vocab.get_itos()[30000]))\n"],"metadata":{"id":"fODc_Npv1KT8","executionInfo":{"status":"ok","timestamp":1642084806874,"user_tz":300,"elapsed":5,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#define pipelines\n","def text_pipeline(x, max_size=500):\n","   text = tokenizer(x)\n","   \n","   # reduce vocab size\n","   pruned_text = []\n","   for token in text:\n","     if text_vocab.get_stoi()[token] >= 30000:\n","       token = '<unk>'\n","     pruned_text.append(token)\n","   \n","   # pad sequence or truncate\n","   if len(pruned_text) <= max_size:\n","     pruned_text += ['<pad>'] * (max_size - len(pruned_text))\n","   else:\n","     pruned_text = pruned_text[0:max_size]\n","   return text_vocab(pruned_text)\n","\n","label_pipeline = lambda x: (0 if (x == 'neg') else 1)"],"metadata":{"id":"_VWkiK8T0orQ","executionInfo":{"status":"ok","timestamp":1642087637352,"user_tz":300,"elapsed":147,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# test pipelines\n","print(text_vocab.get_itos()[29999])\n","print(text_vocab.get_itos()[30000])\n","print(text_pipeline('hello, I saw the wanderings waned'))\n","print(len(text_pipeline('hello, I saw the wanderings waned')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Clf31dYj8Uz5","executionInfo":{"status":"ok","timestamp":1642087639580,"user_tz":300,"elapsed":1524,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}},"outputId":"52a6841c-a76f-4595-d201-f6958fa0f3cb"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["wanderings\n","waned\n","[4645, 4, 13, 220, 2, 29999, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","500\n"]}]},{"cell_type":"markdown","source":["To complete the data preparation, we need to serve minibatches of a desired size from the underlying dataset. We can use the built-in DataLoader class from PyTorch to sample teh dataset in batches.  Before we do so, we need to define a function, collate_batch, that will tell the DataLoader how to preprocess each batch."],"metadata":{"id":"6A-WoVQeONNW"}},{"cell_type":"code","source":["# define preprocessing\n","def collate_batch(batch):\n","  label_list, text_list = [], [] \n","  for label, review in batch:\n","    label_list.append(label_pipeline(label))\n","    text_list.append(text_pipeline(review))\n","  return (torch.tensor(label_list, dtype=torch.float32),\n","          torch.tensor(text_list, dtype=torch.float32))\n"],"metadata":{"id":"cOSlnzk34DL8","executionInfo":{"status":"ok","timestamp":1642087641687,"user_tz":300,"elapsed":128,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["The collate_batch function simply runs the labels and review strings through each respective pipeline and returns the batch as a tuple of tensors (labels_batch, reviews_batch). Once the collate_fn is defined, we simply load the dataset and configure the dataloaders:"],"metadata":{"id":"qt2kmzmVPGXf"}},{"cell_type":"code","source":[""],"metadata":{"id":"mL2Npj0POtpU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load datasets and create dataloaders for batching\n","from torch.utils.data import DataLoader\n","\n","train_iter, val_iter = IMDB(split=('train','test'))\n","trainloader = DataLoader(train_iter, \n","                         batch_size = 1, \n","                         shuffle=False,\n","                         collate_fn=collate_batch)\n","valloader = DataLoader(val_iter, \n","                       batch_size = 1, \n","                       shuffle=False,\n","                       collate_fn=collate_batch)"],"metadata":{"id":"jPpeIZkj2ZRJ","executionInfo":{"status":"ok","timestamp":1642087663146,"user_tz":300,"elapsed":19736,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# test pipelines and collate_batch()\n","for labels, reviews in trainloader:\n","  print(labels.shape)\n","  print(reviews.shape)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H86aQQb52gAo","executionInfo":{"status":"ok","timestamp":1642087746394,"user_tz":300,"elapsed":46896,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}},"outputId":"83032b58-5bac-4f88-c1eb-f2d13c685614"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1])\n","torch.Size([1, 500])\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"4EDxh4nQ6PvJ","executionInfo":{"status":"ok","timestamp":1642085403733,"user_tz":300,"elapsed":141,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# Build TextClassification model"],"metadata":{"id":"0TADZfe3AuXx"}},{"cell_type":"markdown","source":["Now that the data is ready to go, we'll begin to construct the sentiment analysis model, step by step. First, we'll want to map each word in the input review to a word vector. To do this, we'll utilize an  embedding layer, which, as you may recall from the last chapter, is a simple lookup table that stores an embedding vector that corresponds to each word. Unlike in previous examples, where we treated the learning of the word embeddings as a separate problem (i.e., by building a Skip-Gram model), we'll learn the word embeddings jointly with the sentiment analysis problem by treating the embedding matrix as a matrix of parameters in the full problem. We accomplish this by using the PyTorch primitives for managing embeddings (remember that input represents one full minibatch at a time, not just one movie review vector):"],"metadata":{"id":"qTIOsxspPiMS"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"X9TYzEQZBBTV","executionInfo":{"status":"ok","timestamp":1642087776427,"user_tz":300,"elapsed":144,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["embedding = nn.Embedding(\n","                      num_embeddings=30000,\n","                      embedding_dim=512,\n","                      padding_idx=text_vocab.get_stoi()['<pad>'])"],"metadata":{"id":"cFeRlnasBI7V","executionInfo":{"status":"ok","timestamp":1642087777380,"user_tz":300,"elapsed":351,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["#test embedding\n","emb = embedding(torch.randint(high=29999,size=(4,500)))\n","emb.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pGMoB_4EyRk","executionInfo":{"status":"ok","timestamp":1642088149868,"user_tz":300,"elapsed":150,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}},"outputId":"9f4364ea-b592-41f8-ae7a-e13917b9a788"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 500, 512])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["class TextClassifier(nn.Module):\n","  def __init__(self):\n","    super(TextClassifier,self).__init__()\n","    self.layer_1 = nn.Embedding(\n","                      num_embeddings=30000,\n","                      embedding_dim=512,\n","                      padding_idx=1)                      \n","    self.layer_2 = nn.Sequential(\n","                      nn.LSTMCell(input_size=512, hidden_size=512),\n","                      nn.Dropout(p=0.5))\n","    self.layer_3 = nn.Sequential(\n","                      nn.Linear(512, 2),\n","                      nn.Sigmoid(),\n","                      nn.BatchNorm1d(2))\n","  def forward(self, x):\n","    x = self.layer_1(x)\n","    x = self.layer_2(x)\n","    return self.layer_3(x)\n","\n"],"metadata":{"id":"Jv7SjkPz7TlY","executionInfo":{"status":"ok","timestamp":1642088176355,"user_tz":300,"elapsed":132,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Training Loop"],"metadata":{"id":"9ZkangJ-D_2-"}},{"cell_type":"code","source":["import torch.optim as optim "],"metadata":{"id":"s1n9XtjL69j5","executionInfo":{"status":"ok","timestamp":1642086880828,"user_tz":300,"elapsed":153,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["N_EPOCHS = 10\n","model = TextClassifier()\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(classifier.parameters())\n","\n","for epoch in range(N_EPOCHS):\n","  running_loss = 0\n","  for labels, inputs in trainloader:\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = loss_fn(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    running_loss += loss.item()\n","  print(f'Epoch: {epoch} Loss: {loss.item()}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":416},"id":"jyrhBxngD78A","executionInfo":{"status":"error","timestamp":1642086911461,"user_tz":300,"elapsed":29801,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}},"outputId":"e307ae9a-d2c6-4c47-9eed-d4cbe91fb7e1"},"execution_count":63,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-63-6835a98b982d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-61-9393bada2b4d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m                       nn.BatchNorm1d(2))\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"uztcNpK-D75G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"bLIMBEvZD72O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_X-HG3COD7wM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"rwvMieXCD7tV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"oEBCgvGTD7qd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Archive"],"metadata":{"id":"ysccXDkDD9MW"}},{"cell_type":"code","source":[""],"metadata":{"id":"LxAU6eYLD7ng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build dataloader from iterable dataset, shuffle must be False\n","# use collate_fn for preprocessing pipeline\n","def collate_batch(batch):\n","  #for label, text in batch\n","  #return label_batch, text_batch\n","  \n","\n","trainloader = DataLoader(train_iter, \n","                         batch_size = 100, \n","                         shuffle=False, \n","                         collate_fn=collate_batch)"],"metadata":{"id":"XODP38KTzxE6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pass in list of tokens, get indices\n","print(text_vocab(['here', 'we', 'go']))\n","# pass in index, get tokens\n","print(text_vocab.get_itos()[0:10])\n","\n"],"metadata":{"id":"jdBlM5h2uzYa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Limiting vocab for 30000 tokens is a pain\n","see https://github.com/pytorch/text/blob/main/torchtext/vocab/vocab_factory.py"],"metadata":{"id":"zDd-QKSBzVK-"}},{"cell_type":"code","source":["from collections import Counter\n","from torchtext.vocab import vocab\n","from typing import Dict, Iterable, Optional, List\n","from collections import Counter, OrderedDict\n","\n","def build_vocab(iterator: Iterable, tokenizer, max_size: int = 30000,\n","                min_freq: int = 1, specials: Optional[List[str]] = None, \n","                special_first: bool = True) -> Vocab:\n","\n","    counter = Counter()\n","    for _, text in iterator:\n","        counter.update(tokenizer(text))\n","\n","    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[0])\n","    sorted_by_freq_tuples.sort(key=lambda x: x[1], reverse=True)\n","    if len(sorted_by_freq_tuples) > max_size:\n","      sorted_by_freq_tuples = sorted_by_freq_tuples[0:max_size] \n","    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n","\n","    word_vocab = vocab(ordered_dict, min_freq=min_freq)\n","    return word_vocab"],"metadata":{"id":"7BNJha9Xr7et"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_vocab = build_vocab(train_iter, tokenizer, specials = ['<unk>'])\n","print(len(word_vocab))\n","print(word_vocab.get_itos()[0:10])"],"metadata":{"id":"GBnV4XccqgmK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_iter = IMDB(split='train')\n","trainX = [torch.tensor(text_vocab(tokenizer(text))) for _, text in train_iter]\n","# list of 25000 variable size tensors"],"metadata":{"id":"jcTguETh4idS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainX_pad = pad_sequence(trainX, batch_first=True)"],"metadata":{"id":"mooIgeUN8XWU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_dict = {'neg': 0, 'pos': 1}\n","train_iter = IMDB(split='train')\n","trainY = torch.tensor([label_dict[tag] for tag, _ in train_iter])"],"metadata":{"id":"KotC7Y9Q9yZG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainX_pad.shape, len(trainY)"],"metadata":{"id":"J4Z6ip_53upD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IMDBDataset():\n","    def __init__(self, X, Y):\n","        self.num_examples = len(X)\n","        self.inputs = X\n","        self.tags = Y\n","        self.ptr = 0\n","\n","    def minibatch(self, size):\n","        ret = None\n","        if self.ptr + size < len(self.inputs):\n","            ret = self.inputs[self.ptr:self.ptr+size],self.tags[self.ptr:self.ptr+size]\n","        else:\n","            ret = np.concatenate((self.inputs[self.ptr:],\n","                  self.inputs[:size-len(\n","                  self.inputs[self.ptr:])])),\n","                  np.concatenate((self.tags[self.ptr:],\n","                  self.tags[:size-len(\n","                  self.tags[self.ptr:])]))\n","        self.ptr = (self.ptr + size) % len(self.inputs)\n","        return ret"],"metadata":{"id":"OvKO4wpImf3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = IMDBDataset(trainX, trainY)\n","val = IMDBDataset(testX, testY)"],"metadata":{"id":"IxNhUCh9RdCJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define LSTM Model for Sentiment Analysis"],"metadata":{"id":"BEZQ-djza7iY"}},{"cell_type":"code","source":["embedding = nn.Embedding(emb_size = 30000, \n","                         in_size = 512)"],"metadata":{"id":"5uF75S-O5_l5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class SentimentLSTM(nn.Module):\n","  def __init__(self, emb_size, in_size, hidden_size, keep_prob):\n","    super(SentimentLSTM, self).__init__()\n","    self.embedding = nn.Embedding(emb_size, in_size)\n","    self.lstm = nn.LSTM(in_size,\n","                   hidden_size = hidden_size, \n","                   num_layers=2,\n","                   dropout = keep_prob)\n","    self.out_layer = nn.Linear(512,2)\n","\n","  def forward(self, x):\n","    x = self.lstm(self.embedding(x))\n","    return self.out_layer(x)\n","    "],"metadata":{"id":"1zysUtckRdx4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lstm = SentimentLSTM(emb_size = 30000, \n","                     in_size = 512, \n","                     hidden_size = 512, \n","                     keep_prob = 0.5) "],"metadata":{"id":"Z9lr0q5QEGY5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training the LSTM"],"metadata":{"id":"D7Y0oinbFoSU"}},{"cell_type":"markdown","source":["## TO-DO: run this for new pytorch code"],"metadata":{"id":"lRjh27UBFuAm"}},{"cell_type":"code","source":["# Parameters\n","training_epochs = 1000\n","batch_size = 32\n","display_step = 1\n","\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), \n","                       lr=0.001, \n","                       betas=(0.9,0.999), \n","                       eps=1e-08)\n","writer = SummaryWriter()\n","\n","for epoch in range(training_epochs):\n","  running_loss = 0\n","  for inputs, labels in trainloader:\n","    optimizer.zero_grad()\n","    outputs = model(inputs)\n","    loss = loss_fn(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    running_loss += loss\n","  \n","\n","  writer.add_scalar('Loss/train', running_loss/len(trainloader), epoch)\n","  #if (epoch % 100 == 0):\n","  print(f'Epoch: {epoch} Loss: {running_loss/len(trainloader)}')"],"metadata":{"id":"LZzTO8FiB5PZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(dataloader):\n","    model.train()\n","    total_acc, total_count = 0, 0\n","    log_interval = 500\n","    start_time = time.time()\n","\n","    for idx, (label, text, offsets) in enumerate(dataloader):\n","        optimizer.zero_grad()\n","        predicted_label = model(text, offsets)\n","        loss = criterion(predicted_label, label)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n","        optimizer.step()\n","        total_acc += (predicted_label.argmax(1) == label).sum().item()\n","        total_count += label.size(0)\n","        if idx % log_interval == 0 and idx > 0:\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches '\n","                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n","                                              total_acc/total_count))\n","            total_acc, total_count = 0, 0\n","            start_time = time.time()\n","\n","def evaluate(dataloader):\n","    model.eval()\n","    total_acc, total_count = 0, 0\n","\n","    with torch.no_grad():\n","        for idx, (label, text, offsets) in enumerate(dataloader):\n","            predicted_label = model(text, offsets)\n","            loss = criterion(predicted_label, label)\n","            total_acc += (predicted_label.argmax(1) == label).sum().item()\n","            total_count += label.size(0)\n","    return total_acc/total_count"],"metadata":{"id":"hTDv4LyqCM0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data.dataset import random_split\n","from torchtext.data.functional import to_map_style_dataset\n","# Hyperparameters\n","EPOCHS = 10 # epoch\n","LR = 5  # learning rate\n","BATCH_SIZE = 64 # batch size for training\n","\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n","total_accu = None\n","train_iter, test_iter = AG_NEWS()\n","train_dataset = to_map_style_dataset(train_iter)\n","test_dataset = to_map_style_dataset(test_iter)\n","num_train = int(len(train_dataset) * 0.95)\n","split_train_, split_valid_ = \\\n","    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n","\n","train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n","                              shuffle=True, collate_fn=collate_batch)\n","test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                             shuffle=True, collate_fn=collate_batch)\n","\n","for epoch in range(1, EPOCHS + 1):\n","    epoch_start_time = time.time()\n","    train(train_dataloader)\n","    accu_val = evaluate(valid_dataloader)\n","    if total_accu is not None and total_accu > accu_val:\n","      scheduler.step()\n","    else:\n","       total_accu = accu_val\n","    print('-' * 59)\n","    print('| end of epoch {:3d} | time: {:5.2f}s | '\n","          'valid accuracy {:8.3f} '.format(epoch,\n","                                           time.time() - epoch_start_time,\n","                                           accu_val))\n","    print('-' * 59) "],"metadata":{"id":"vmu4V1S_DVvh"},"execution_count":null,"outputs":[]}]}