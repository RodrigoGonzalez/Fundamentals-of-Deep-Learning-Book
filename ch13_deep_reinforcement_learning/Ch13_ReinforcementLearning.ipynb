{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ch14_RL.ipynb","provenance":[],"authorship_tag":"ABX9TyOWS0IMTeMFN5uzHnqVGLSG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# TODO \n","- (Later)Add tests for functions"],"metadata":{"id":"-yGp0pP7LOgu"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","import gym\n","import tqdm\n","import torch\n","import torch.nn as nn"],"metadata":{"id":"p_lTuUjY_QLg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_naive_returns(rewards):\n","    \"\"\" Calculates a list of naive returns given a \n","    list of rewards.\"\"\"\n","    total_returns = np.zeros(len(rewards))\n","    total_return = 0.0\n","    for t in range(len(rewards), 0):\n","        total_return = total_return + reward\n","        total_returns[t] = total_return\n","    return total_returns\n","\n","\n","def discount_rewards(rewards, gamma=0.98):\n","    discounted_returns = [0 for _ in rewards]\n","    discounted_returns[-1] = rewards[-1]\n","    # iterate backwards\n","    for t in range(len(rewards)-2, -1, -1): \n","        discounted_returns[t] = (rewards[t] + \n","              discounted_returns[t+1]*gamma)\n","    return discounted_returns\n","\n","def epsilon_greedy_action(action_distribution, \n","                          epsilon=1e-1):\n","    if random.random() < epsilon:\n","        return np.argmax(np.random.random(\n","           action_distribution.shape))\n","    else:\n","        return np.argmax(action_distribution)\n","\n","def epsilon_greedy_action_annealed(action_distribution,\n","                                   percentage, \n","                                   epsilon_start=1.0, \n","                                   epsilon_end=1e-2):\n","    annealed_epsilon = (epsilon_start*(1.0-percentage) + \n","                        epsilon_end*percentage)\n","    if random.random() < annealed_epsilon:\n","        return np.argmax(np.random.random(\n","          action_distribution.shape))\n","    else:\n","        return np.argmax(action_distribution)"],"metadata":{"id":"FzfQoWzz_RkV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Pole-Cart with Policy Gradients"],"metadata":{"id":"PyufQkgboZY3"}},{"cell_type":"markdown","source":["## Creating an Agent"],"metadata":{"id":"1BR54huUoHOG"}},{"cell_type":"code","metadata":{"id":"UjQX09lMWloJ","executionInfo":{"status":"error","timestamp":1642172590753,"user_tz":300,"elapsed":134,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}},"colab":{"base_uri":"https://localhost:8080/","height":132},"outputId":"bd67540b-c99a-4ade-917f-498c0aab43d8"},"source":["class PGAgent(object):\n","    def __init__(self, state_size, num_actions, \n","                 hidden_size, \n","                 learning_rate=1e-3, \n","                 explore_exploit_setting= \\\n","                 'epsilon_greedy_annealed_1.0->0.001'):\n","        self.state_size = state_size\n","        self.num_actions = num_actions\n","        self.hidden_size = hidden_size\n","        self.learning_rate = learning_rate\n","        self.explore_exploit_setting = \\\n","                        explore_exploit_setting\n","        self.build_model()\n","        self.build_training()\n","\n","\n","    def build_model(self):\n","      self.model = torch.nn.Sequential(\n","        nn.Linear(self.state_size, self.hidden_size),\n","        nn.Linear(self.hidden_size, self.hidden_size),\n","        nn.Linear(self.hidden_size, self.num_actions),\n","        nn.Softmax())\n","\n","    def train(self, state, action_input, reward_input):\n","        self.output = self.model(state)\n","        # Select the logits related to the action taken\n","        self.output_index_for_actions = (torch.range(0, \n","                  self.output.size(dim=0)-1) * \\\n","                  self.output.size(dim=1)) + action_input\n","        self.logits_for_actions = torch.gather(\n","            torch.reshape(self.output, (-1,)),0,\n","            self.output_index_for_actions)\n","        self.loss = - torch.mean(\n","            torch.log(self.logits_for_actions) * \n","            self.reward_input)\n","        self.loss.backward()\n","        self.optimizer = optim.AdamOptimizer(\n","            self.model.parameters())\n","        self.optimizer.step()\n","        self.loss.zero_grad()\n","        return self.loss.item()\n","        \n","\n","\n","    def sample_action_from_distribution(self, \n","                                        action_distribution, \n","                                        epsilon_percentage):\n","        # Choose an action based on the action probability\n","        # distribution and an explore vs exploit\n","        if self.explore_exploit_setting == \\\n","          'greedy':\n","              action = greedy_action(\n","                  action_distribution)\n","        elif self.explore_exploit_setting == \\\n","          'epsilon_greedy_0.05':\n","              action = epsilon_greedy_action(\n","                  action_distribution,0.05)\n","        elif self.explore_exploit_setting == \\\n","          'epsilon_greedy_0.25':\n","              action = epsilon_greedy_action(\n","                  action_distribution,0.25)\n","        elif self.explore_exploit_setting == \\ \n","          'epsilon_greedy_0.50':\n","              action = epsilon_greedy_action(\n","                  action_distribution,0.50)\n","        elif self.explore_exploit_setting == \\ \n","          'epsilon_greedy_0.90':\n","              action = epsilon_greedy_action(\n","                  action_distribution,0.90)\n","        elif self.explore_exploit_setting == \\\n","          'epsilon_greedy_annealed_1.0->0.001':\n","              action = epsilon_greedy_action_annealed(\n","                  action_distribution, \n","                  epsilon_percentage, 1.0,0.001)\n","        elif self.explore_exploit_setting == \\\n","          'epsilon_greedy_annealed_0.5->0.001':\n","              action = epsilon_greedy_action_annealed(\n","                  action_distribution, \n","                  epsilon_percentage, 0.5, 0.001)\n","        elif self.explore_exploit_setting == \\\n","          'epsilon_greedy_annealed_0.25->0.001':\n","              action = epsilon_greedy_action_annealed(\n","                  action_distribution, \n","                  epsilon_percentage, 0.25, 0.001)\n","        return action\n","\n","    def predict_action(self, state, epsilon_percentage):\n","        action_distribution = self.model(state)[0]\n","        action = self.sample_action_from_distribution(\n","            action_distribution, epsilon_percentage)\n","        return action"],"execution_count":22,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-c86ff0992df5>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    elif self.explore_exploit_setting == \\\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected character after line continuation character\n"]}]},{"cell_type":"markdown","source":["## Keeping Track of History"],"metadata":{"id":"TfvvUskNohYl"}},{"cell_type":"code","source":["class EpisodeHistory(object):\n","\n","    def __init__(self):\n","        self.states = []\n","        self.actions = []\n","        self.rewards = []\n","        self.state_primes = []\n","        self.discounted_returns = []\n","\n","    def add_to_history(self, state, action, reward, \n","      state_prime):\n","        self.states.append(state)\n","        self.actions.append(action)\n","        self.rewards.append(reward)\n","        self.state_primes.append(state_prime)\n","\n","\n","class Memory(object):\n","\n","    def __init__(self):\n","        self.states = []\n","        self.actions = []\n","        self.rewards = []\n","        self.state_primes = []\n","        self.discounted_returns = []\n","\n","    def reset_memory(self):\n","        self.states = []\n","        self.actions = []\n","        self.rewards = []\n","        self.state_primes = []\n","        self.discounted_returns = []\n","\n","    def add_episode(self, episode):\n","        self.states += episode.states\n","        self.actions += episode.actions\n","        self.rewards += episode.rewards\n","        self.discounted_returns += episode.discounted_returns"],"metadata":{"id":"YAGE0IJR_uX_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Policy Gradient Main Function"],"metadata":{"id":"xsRN8A9Momgd"}},{"cell_type":"code","source":["# Configure Settings\n","total_episodes = 5000\n","total_steps_max = 10000\n","epsilon_stop = 3000\n","train_frequency = 8\n","max_episode_length = 500\n","render_start = -1\n","should_render = False\n","\n","explore_exploit_setting = 'epsilon_greedy_annealed_1.0->0.001'\n","\n","env = gym.make('CartPole-v0')\n","state_size = env.observation_space.shape[0]  # 4 for \n","                                              # CartPole-v0\n","num_actions = env.action_space.n  # 2 for CartPole-v0\n","\n","solved = False\n","agent = PGAgent(state_size=state_size,\n","                num_actions=num_actions,\n","                hidden_size=16, \n","                explore_exploit_setting= \\\n","                  explore_exploit_setting)"],"metadata":{"id":"_dhvNmQn4lh0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["episode_rewards = []\n","batch_losses = []\n","\n","global_memory = Memory()\n","steps = 0\n","for i in range(total_episodes):\n","  state = torch.tensor(env.reset(), dtype=torch.float32)\n","  episode_reward = 0.0\n","  episode_history = EpisodeHistory()\n","  epsilon_percentage = float(min(i/float(\n","    epsilon_stop), 1.0))\n","\n","  for j in range(max_episode_length):\n","      action = agent.predict_action(state, epsilon_percentage)\n","        \n","      state_prime, reward, terminal, _ = env.step(action)\n","      state_prime = torch.tensor(state_prime, \n","                                 dtype=torch.float32)\n","      \n","      # if (render_start > 0 and i > \n","      #   render_start and should_render) \\\n","      #     or (solved and should_render):\n","      #     env.render()\n","      episode_history.add_to_history(\n","          state, action, reward, state_prime)\n","      state = state_prime\n","      episode_reward += reward\n","      steps += 1\n","      \n","      if terminal:\n","          episode_history.discounted_returns = \\\n","            discount_rewards(episode_history.rewards)\n","          global_memory.add_episode(episode_history)\n","\n","          if np.mod(i, train_frequency) == 0:\n","              reward_input = torch.tensor(\n","                  global_memory.discounted_returns,\n","                  dtype=torch.float32)\n","              action_input = torch.tensor(\n","                  global_memory.actions,\n","                  dtype=torch.float32)\n","              state = torch.stack(global_memory.states)\n","\n","              # train step \n","              batch_loss = agent.train(state, \n","                                       action_input, \n","                                       reward_input)\n","              batch_losses.append(batch_loss)\n","              global_memory.reset_memory()\n","\n","          episode_rewards.append(episode_reward)\n","          break\n","\n","          if i % 10:\n","              if torch.mean(episode_rewards[:-100]) > 100.0:\n","                  solved = True\n","              else:\n","                  solved = False\n","          print('Solved:', solved, 'Mean Reward', \n","                torch.mean(episode_rewards[:-100]))\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"LS7KWo07_61i","executionInfo":{"status":"error","timestamp":1642172675244,"user_tz":300,"elapsed":395,"user":{"displayName":"Joe Papa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00487850786587503652"}},"outputId":"2ca0a34a-bbd0-47f4-be39-13ce21224f69"},"execution_count":23,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-276f94db141a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mglobal_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Memory' is not defined"]}]},{"cell_type":"code","source":["action_input.shape"],"metadata":{"id":"BX813LZhlMgr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Q-Learning and Deep Q-Networks"],"metadata":{"id":"ia6irv4qo2L0"}},{"cell_type":"markdown","source":["## Playing Breakout wth DQN"],"metadata":{"id":"f1uFTHZmo5CF"}},{"cell_type":"code","source":["class DQNAgent(object):\n","\n","    def __init__(self, num_actions,\n","                 learning_rate=1e-3, history_length=4,\n","                 screen_height=84, screen_width=84, \n","                 gamma=0.99):\n","        self.num_actions = num_actions\n","        self.learning_rate = learning_rate\n","        self.history_length = history_length\n","        self.screen_height = screen_height\n","        self.screen_width = screen_width\n","        self.gamma = gamma\n","        self.optimizer = optim.Adam()\n","\n","        self.build_prediction_network()\n","        self.build_target_network()\n","        self.build_training()\n","\n","    def build_prediction_network(self):\n","        self.model_predict = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=8 , stride=4),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.Flatten(),\n","            nn.Linear(64, 512),\n","            nn.Linear(512, self.num_actions)\n","            )\n","        self.q_action = torch.argmax(self.model_predict, dim=1)\n","\n","\n","    def build_target_network(self):\n","        self.model_target = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=8 , stride=4),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.Flatten(),\n","            nn.Linear(64, 512),\n","            nn.Linear(512, self.num_actions)\n","            )\n","        self.target_q_action = torch.argmax(self.target_q, \n","                                         dim=1)\n","        \n","    def update_target_q_weights(self):\n","        self.optimizer_predict.step()\n","\n","    def sample_and_train_pred(self, replay_table, batch_size):\n","\n","        s_t, action, reward, s_t_plus_1, terminal = \\\n","          replay_table.sample_batch(batch_size)\n","        q_t_plus_1 = self.model_target(\n","            {self.target_s_t: \n","              s_t_plus_1})\n","\n","\n","        terminal = np.array(terminal) + 0.\n","        max_q_t_plus_1 = torch.max(q_t_plus_1, \n","          axis=1)\n","        target_q_t = (1. - terminal) * self.gamma * \n","              max_q_t_plus_1 + reward\n","        _, q_t, loss = self.train_model(self.action, \n","                                        self.s_t, \n","                                        self.target_q_t)\n","\n","        return q_t\n","\n","\n","    def build_training(self):\n","        action_one_hot = torch.nn.functional.one_hot(\n","            self.action, num_classes=self.num_actions)\n","        \n","        q_of_action = torch.sum(self.q_t * action_one_hot, \n","                                dim=1)\n","\n","        self.delta = (self.target_q_t - q_of_action)\n","        self.loss = torch.mean(self.clip_error(self.delta),\n","                               dim=0)\n","\n","        self.optimizer = torch.optim.AdamOptimizer(\n","            self.model_predict.parameters(),\n","            learning_rate=self.learning_rate)\n","        self.train_step = self.optimizer.step()\n","\n","    def sample_action_from_distribution(self, \n","      action_distribution, epsilon_percentage):\n","        # Choose an action based on the action probability \n","        # distribution\n","        action = epsilon_greedy_action_annealed(\n","            action_distribution, epsilon_percentage)\n","        return action\n","\n","    def predict_action(self, state, epsilon_percentage):\n","        action_distribution = train_target()\n","        action = self.sample_action_from_distribution(\n","            action_distribution, epsilon_percentage)\n","        return action\n","\n","    def process_state_into_stacked_frames(self, frame, \n","      past_frames, past_state=None):\n","        full_state = np.zeros(\n","            (self.history_length, self.screen_width, \n","              self.screen_height))\n","\n","        if past_state is not None:\n","            for i in range(len(past_state)-1):\n","                full_state[i, :, :] = past_state[i+1, :, :]\n","            full_state[-1, :, :] = self.preprocess_frame(\n","                  frame,\n","                  (self.screen_width, self.screen_height))\n","        else:\n","            all_frames = past_frames + [frame]\n","            for i, frame_f in enumerate(all_frames):\n","                full_state[i, :, :] = self.preprocess_frame(\n","                    frame_f, \n","                    (self.screen_width, self.screen_height))\n","        return full_state\n","\n","    def to_grayscale(self, x):\n","        return np.dot(x[...,:3], [0.299, 0.587, 0.114])\n","\n","    def clip_error(self, x):\n","      try:\n","        return torch.select(torch.abs(x) < 1.0, \n","                            0.5 * torch.square(x), \n","                            torch.abs(x) - 0.5)\n","      except:\n","        return torch.where(torch.abs(x) < 1.0, \n","                           0.5 * torch.square(x), \n","                           torch.abs(x) - 0.5)\n","\n","    def preprocess_frame(self, im, shape):\n","        cropped = im[16:201,:]\n","        grayscaled = self.to_grayscale(cropped)\n","        resized = imresize(grayscaled, shape, \n","                           'nearest').astype('float32')\n","        mean, std = 40.45, 64.15\n","        frame = (resized-mean)/std\n","        return frame"],"metadata":{"id":"rkyNqr10AKvv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implementing Experience Replay"],"metadata":{"id":"T4a41ZezpFEo"}},{"cell_type":"code","source":["class ExperienceReplayTable(object):\n","\n","    def __init__(self, table_size=50000):\n","        self.states = []\n","        self.actions = []\n","        self.rewards = []\n","        self.state_primes = []\n","        self.terminals = []\n","\n","        self.table_size = table_size\n","\n","    def add_episode(self, episode):\n","        self.states += episode.states\n","        self.actions += episode.actions\n","        self.rewards += episode.rewards\n","        self.state_primes += episode.state_primes\n","        self.terminals += episode.terminals\n","\n","        self.purge_old_experiences()\n","\n","    def purge_old_experiences(self):\n","        while len(self.states) > self.table_size:\n","            self.states.pop(0)\n","            self.actions.pop(0)\n","            self.rewards.pop(0)\n","            self.state_primes.pop(0)\n","\n","    def sample_batch(self, batch_size):\n","        s_t, action, reward = [], [], []\n","        s_t_plus_1, terminal = [], []\n","        rands = np.arange(len(self.states))\n","        np.random.shuffle(rands)\n","        rands = rands[:batch_size]\n","\n","        for r_i in rands:\n","            s_t.append(self.states[r_i])\n","            action.append(self.actions[r_i])\n","            reward.append(self.rewards[r_i])\n","            s_t_plus_1.append(self.state_primes[r_i])\n","            terminal.append(self.terminals[r_i])\n","        return (np.array(s_t), np.array(action), \n","                np.array(reward), np.array(s_t_plus_1), \n","                np.array(terminal)\n"],"metadata":{"id":"-npfMldDAcGk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DQN Main Loop"],"metadata":{"id":"FDtrjMY0pK4e"}},{"cell_type":"code","source":["# Download Atari ROMS for Breakout\n","! wget http://www.atarimania.com/roms/Roms.rar\n","! mkdir /content/ROM/\n","! unrar e /content/Roms.rar /content/ROM/\n","! python -m atari_py.import_roms /content/ROM/"],"metadata":{"id":"HTcCPhtv7Agz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Configure Settings\n","learn_start = 15000\n","# learn_start = 1\n","scale = 30\n","total_episodes = 500*scale\n","epsilon_stop = 200*scale\n","# epsilon_stop = 20\n","train_frequency = 4\n","target_frequency = 1000\n","# batch_size = 64\n","batch_size = 32\n","max_episode_length = 100000\n","render_start = 10\n","should_render = False\n","\n","env = gym.make('Breakout-v4')\n","num_actions = env.action_space.n"],"metadata":{"id":"RmB7GQai6Eoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["solved = False\n","\n","agent = DQNAgent(num_actions=num_actions, \n","                 learning_rate=1e-4, \n","                 history_length=4,\n","                 gamma=0.98)\n","    \n","\n","episode_rewards = []\n","q_t_list = []\n","batch_losses = []\n","past_frames_last_time = None\n","\n","replay_table = ExperienceReplayTable()\n","global_step_counter = 0\n","for i in range(total_episodes):\n","    frame = env.reset()\n","    past_frames = [copy.deepcopy(frame) \\\n","                   for _ in range(agent.history_length-1)]\n","    state = agent.process_state_into_stacked_frames(\n","        frame, past_frames, past_state=None)\n","    episode_reward = 0.0\n","    episode_history = EpisodeHistory()\n","    epsilon_percentage = float(min(i/float(\n","      epsilon_stop), 1.0))\n","    for j in range(max_episode_length):\n","        action = agent.predict_action(state, \n","                                      epsilon_percentage)\n","        if global_step_counter < learn_start:\n","            action = torch.argmax(\n","                torch.random.random((agent.num_actions)))\n","\n","        reward = 0\n","\n","        frame_prime, reward, terminal, _ = env.step(action)\n","        if terminal == True:\n","            reward -= 1\n","\n","        state_prime = \\\n","        agent.process_state_into_stacked_frames(\n","            frame_prime, \n","            past_frames, \n","            past_state=state)\n","\n","        past_frames.append(frame_prime)\n","        past_frames = past_frames[len(past_frames) - \\\n","                                  agent.history_length:]\n","\n","        past_frames_last_time = past_frames\n","\n","        if ((i > render_start) and should_render or \n","            (solved and should_render)):\n","              env.render()\n","        episode_history.add_to_history(\n","            state, action, reward, state_prime, terminal)\n","        state = state_prime\n","        episode_reward += reward\n","        global_step_counter += 1\n","\n","        if global_step_counter > learn_start:\n","            if global_step_counter % train_frequency == 0:\n","                q_t = agent.sample_and_train_pred(\n","                    replay_table, batch_size)\n","                q_t_list.append(q_t)\n","\n","                if global_step_counter % target_frequency == 0:\n","                    agent.update_target_q_weights()\n","\n","\n","        if j == (max_episode_length - 1):\n","            terminal = True\n","\n","        if terminal:\n","            replay_table.add_episode(episode_history)\n","            episode_rewards.append(episode_reward)\n","            break\n","\n","    if i % 50 == 0:\n","        ave_reward = np.mean(episode_rewards[-100:])\n","        ep_percent = float(min(i/float(epsilon_stop), 1.0))\n","        print(\"Reward Stats (min, max, median, mean): \", \n","              np.min(episode_rewards[-100:]), \n","              np.max(episode_rewards[-100:]), \n","              np.median(episode_rewards[-100:]), \n","              np.mean(episode_rewards[-100:]))\n","        print(\"Global Stats (ep_percent, global_step_counter): \", \n","              ep_percent, global_step_counter)\n","        if q_t_list:\n","          print(\"Qt Stats (min, max, median, mean): \", \n","                np.min(q_t_list[-1000:]), \n","                np.max(q_t_list[-100:]), \n","                np.median(q_t_list[-100:]), \n","                np.mean(q_t_list[-100:]))\n","        if ave_reward > 50.0:\n","            solved = True\n","            print('solved')\n","        else:\n","            solved = False"],"metadata":{"id":"spcwGUJ27g8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"HYZRByFQ_Obw"},"execution_count":null,"outputs":[]}]}